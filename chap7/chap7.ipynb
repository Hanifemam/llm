{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d9120",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
    "!pip install langchain_community\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1a085",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "llm = LlamaCpp(model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "               n_gpu_layers=-1,\n",
    "               n_ctx=2048,\n",
    "               seed=42,\n",
    "               verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c25289",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"Hi! I am han. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be2b93d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"<s><|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa5da9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! I am han. What is 1 + 1\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef4693",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"<s><|user|>\n",
    "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(template=template, input_variables=[\"summary\", \"title\", \"character\"])\n",
    "story_chain = story_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec895ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 2 & 3: Title Generation Chain\n",
    "title_template = \"\"\"<s><|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "title_prompt = PromptTemplate(\n",
    "    template=title_template,\n",
    "    input_variables=[\"summary\"]\n",
    ")\n",
    "title_chain = title_prompt | llm\n",
    "\n",
    "# 4 & 5: Character Description Chain\n",
    "character_template = \"\"\"<s><|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "character_prompt = PromptTemplate(\n",
    "    template=character_template,\n",
    "    input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "character_chain = character_prompt | llm\n",
    "\n",
    "# 6 & 7: Story Generation Chain\n",
    "story_template = \"\"\"<s><|user|>\n",
    "Create a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=story_template,\n",
    "    input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "story_chain = story_prompt | llm\n",
    "\n",
    "print(\"All PromptTemplate objects and LCEL chains defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d0c93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{input_prompt}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc4b3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"<s><|user|>Current conversation:{chat_history} {input_prompt}<|end|> <|assistant|>\"\"\" \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input_prompt\", \"chat_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2ff3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda _: history,\n",
    "    input_messages_key=\"input_prompt\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619fb1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"input_prompt\": \"Tell me about Madrid's 2022 UCL win.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c794180",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "def get_windowed_history(_):\n",
    "    # keep only the last 6 messages (3 turns)\n",
    "    history.messages[:] = history.messages[-6:]\n",
    "    return history\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_windowed_history,\n",
    "    input_messages_key=\"input_prompt\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3657233",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"input_prompt\": \"Tell me about Madrid's 2022 UCL win.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
