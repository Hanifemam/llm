{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d9120",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
    "!pip install langchain_community\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1a085",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "llm = LlamaCpp(model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "               n_gpu_layers=-1,\n",
    "               n_ctx=2048,\n",
    "               seed=42,\n",
    "               verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c25289",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"Hi! I am han. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be2b93d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"<s><|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa5da9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! I am han. What is 1 + 1\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef4693",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"<s><|user|>\n",
    "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(template=template, input_variables=[\"summary\", \"title\", \"character\"])\n",
    "story_chain = story_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec895ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 2 & 3: Title Generation Chain\n",
    "title_template = \"\"\"<s><|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "title_prompt = PromptTemplate(\n",
    "    template=title_template,\n",
    "    input_variables=[\"summary\"]\n",
    ")\n",
    "title_chain = title_prompt | llm\n",
    "\n",
    "# 4 & 5: Character Description Chain\n",
    "character_template = \"\"\"<s><|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "character_prompt = PromptTemplate(\n",
    "    template=character_template,\n",
    "    input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "character_chain = character_prompt | llm\n",
    "\n",
    "# 6 & 7: Story Generation Chain\n",
    "story_template = \"\"\"<s><|user|>\n",
    "Create a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=story_template,\n",
    "    input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "story_chain = story_prompt | llm\n",
    "\n",
    "print(\"All PromptTemplate objects and LCEL chains defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d0c93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{input_prompt}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc4b3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"<s><|user|>Current conversation:{chat_history} {input_prompt}<|end|> <|assistant|>\"\"\" \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input_prompt\", \"chat_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2ff3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda _: history,\n",
    "    input_messages_key=\"input_prompt\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619fb1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"input_prompt\": \"Tell me about Madrid's 2022 UCL win.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c794180",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "def get_windowed_history(_):\n",
    "    # keep only the last 6 messages (3 turns)\n",
    "    history.messages[:] = history.messages[-6:]\n",
    "    return history\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_windowed_history,\n",
    "    input_messages_key=\"input_prompt\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3657233",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"input_prompt\": \"Tell me about Madrid's 2022 UCL win.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a77f77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "summary = \"\"             # holds the running summary\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "async def summarize(history_messages):\n",
    "    \"\"\"Build summary using the LLM.\"\"\"\n",
    "    text = \"\\n\".join([f\"{m.type}: {m.content}\" for m in history_messages])\n",
    "    response = llm.invoke(f\"Summarize this conversation:\\n{text}\")\n",
    "    return response.content\n",
    "\n",
    "def get_summary_history(_):\n",
    "    global summary\n",
    "\n",
    "    # Only summarize when the history grows too large\n",
    "    if len(history.messages) > 6:\n",
    "        summary = llm.invoke(\n",
    "            f\"Combine this existing summary:\\n{summary}\\n\\n\"\n",
    "            f\"With this new conversation:\\n\" +\n",
    "            \"\\n\".join(m.content for m in history.messages)\n",
    "        ).content\n",
    "        \n",
    "        history.clear()\n",
    "        history.add_message(AIMessage(content=f\"[Summary so far]: {summary}\"))\n",
    "\n",
    "    return history\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain_with_summary = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_summary_history,\n",
    "    input_messages_key=\"input_prompt\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ab18e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "response = chain_with_summary.invoke(\n",
    "    {\"input_prompt\": \"Explain Madrid's 2022 UCL win.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"s1\"}}\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03e5a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76031b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df18517",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "co = cohere.Client(api_key)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
