{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6238fd3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "# Load an AI-generated image of a puppy playing in the snow\n",
    "puppy_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png\"\n",
    "image = Image.open(urlopen(puppy_path)).convert(\"RGB\")\n",
    "caption = \"a puppy playing in the snow\"\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ddcf2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "model = CLIPModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730560d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01302a69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84366440",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text_embedding = model.get_text_features(**inputs)\n",
    "text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c424b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "processed_image = clip_processor(\n",
    "    text=None, images=image, return_tensors=\"pt\"\n",
    ")[\"pixel_values\"]\n",
    "processed_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485023a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "img = processed_image.squeeze(0)\n",
    "img = img.permute(*torch.arange(img.ndim - 1, -1, -1))\n",
    "img = np.einsum(\"ijk->jik\", img)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8c53f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_embedding = model.get_image_features(processed_image)\n",
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33639e07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_embedding = text_embedding.detach().cpu().numpy()\n",
    "image_embedding = image_embedding.detach().cpu().numpy()\n",
    "score = text_embedding @ image_embedding.T\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4481e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
    "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94508b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "inputs[\"pixel_values\"].shape\n",
    "blip_processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37509eca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Her vocalization was remarkably melodic\"\n",
    "token_ids = blip_processor(image, text=text, return_tensor=\"pt\")\n",
    "token_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\n",
    "tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
